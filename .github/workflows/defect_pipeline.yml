name: Defect Detection Pipeline (Local MLflow)

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  defect-detection:
    runs-on: self-hosted  # Your local Mac self-hosted runne

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python environment (venv)
        shell: bash
        run: |
          python3 -m venv venv
          source venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install mlflow==2.12.1 protobuf==3.20.3
          python -m pip install -r requirements.txt

      - name: (Optional) Clean old MLflow artifacts
        shell: bash
        run: |
          rm -rf mlruns.db mlruns

      - name: Preprocess Data
        shell: bash
        run: |
          source venv/bin/activate
          python preprocess/preprocess.py --config preprocess/config.yaml

      - name: Train Model
        id: train_model_step
        shell: bash
        run: |
          source venv/bin/activate
          export MLFLOW_TRACKING_URI=http://localhost:5000
          TRAINING_RUN_ID=$(python training/train.py --config training/config.yaml --mlflow_config mlflow_config.yaml | grep "mlflow_run_id:" | awk '{print $2}')
          echo "mlflow_run_id: $TRAINING_RUN_ID"
          echo "training_run_id=$TRAINING_RUN_ID" >> $GITHUB_OUTPUT

      - name: Evaluate Model
        shell: bash
        run: |
          source venv/bin/activate
          export MLFLOW_TRACKING_URI=http://localhost:5000
          python evaluation/evaluate.py --config evaluation/config.yaml --mlflow_config mlflow_config.yaml --training_run_id ${{ steps.train_model_step.outputs.training_run_id }}

      - name: Deploy Model (Set 'prod' alias)
        shell: bash
        run: |
          echo "If evaluation is successful, the 'prod' alias will be set for the model."
          echo "This is handled within the evaluation/evaluate.py script."

      - name: Run Inference (Example)
        shell: bash
        run: |
          source venv/bin/activate
          export MLFLOW_TRACKING_URI=http://localhost:5000
          DUMMY_IMAGE_PATH="data/test/ok_front/cast_ok_0_10.jpeg"
          if [ -f "$DUMMY_IMAGE_PATH" ]; then
            python inference/infer.py --config inference/config.yaml --mlflow_config mlflow_config.yaml --image_path "$DUMMY_IMAGE_PATH"
          else
            echo "Dummy image not found at $DUMMY_IMAGE_PATH. Skipping inference example."
          fi
